{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOk9Cg415Q4QPMh06beI/cS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thamil-vani/Image-Steganography-using-deep-neural-networks/blob/main/Stacked_Autoencoder_Final_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIE2P1LCRfmd",
        "outputId": "0eb6286f-0cd0-4878-f3a6-4d21de924518"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuxcXx2iRneR",
        "outputId": "01d010a7-b6f0-4678-8dd9-be429e0d44c3"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
        "#from keras.engine.network import Network\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import keras.backend as K\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import scipy.misc\n",
        "from tqdm import *\n",
        "\n",
        "def loadCover():\n",
        "    X_train=[]\n",
        " \n",
        "    for c in os.listdir(\"/content/gdrive/MyDrive/imagestegan/dataset/cover\"):\n",
        "            img_i = image.load_img(os.path.join(\"/content/gdrive/MyDrive/imagestegan/dataset/cover\", c))\n",
        "            img_i = img_i.resize((64,64))\n",
        "            x = image.img_to_array(img_i)\n",
        "            X_train.append(x)\n",
        "    return np.array(X_train,dtype=float)\n",
        "\n",
        "def loadSecret():\n",
        "    X_train=[]\n",
        "    for c in os.listdir(\"/content/gdrive/MyDrive/imagestegan/dataset/s/secret\"):\n",
        "            img_i = image.load_img(os.path.join(\"/content/gdrive/MyDrive/imagestegan/dataset/s/secret\", c))\n",
        "            img_i = img_i.resize((64,64))\n",
        "            x = image.img_to_array(img_i)\n",
        "            X_train.append(x)\n",
        "    return np.array(X_train,dtype=float)\n",
        "\n",
        "\n",
        "X_train_orig=loadCover()\n",
        "X_test_orig=loadSecret()\n",
        "\n",
        "X_train = X_train_orig/255.\n",
        "X_test = X_test_orig/255.\n",
        "print (\"Number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape)) # Should be (train_size, 64, 64, 3).\n",
        "print (\"Number of training examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_test.shape)) # Should be (train_size, 64, 64, 3).\n",
        "\n",
        "\n",
        "input_S = X_test\n",
        "input_C = X_train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples = 1000\n",
            "X_train shape: (1000, 64, 64, 3)\n",
            "Number of training examples = 1000\n",
            "X_train shape: (1000, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbnGTgnPVFsZ"
      },
      "source": [
        "\n",
        "input_S = X_test\n",
        "input_C = X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i54-5WMtXy5C"
      },
      "source": [
        "# Variable used to weight the losses of the secret and cover images (See paper for more details)\n",
        "beta = 1.0\n",
        "    \n",
        "# Loss for reveal network\n",
        "def rev_loss(s_true, s_pred):\n",
        "    # Loss for reveal network is: beta * |S-S'|\n",
        "    return beta * K.sum(K.square(s_true - s_pred))\n",
        "\n",
        "# Loss for the full model, used for preparation and hidding networks\n",
        "def full_loss(y_true, y_pred):\n",
        "    # Loss for the full model is: |C-C'| + beta * |S-S'|\n",
        "    s_true, c_true = y_true[:,:,:,0:3], y_true[:,:,:,3:6]\n",
        "    s_pred, c_pred = y_pred[:,:,:,0:3], y_pred[:,:,:,3:6]\n",
        "    \n",
        "    s_loss = beta * K.sum(K.square(s_true - s_pred))\n",
        "    c_loss = K.sum(K.square(c_true - c_pred))\n",
        "    \n",
        "    return s_loss + c_loss\n",
        "\n",
        "\n",
        "# Returns the encoder as a Keras model, composed by Preparation and Hiding Networks.\n",
        "def make_encoder(input_size):\n",
        "    input_S = Input(shape=(input_size))\n",
        "    input_C= Input(shape=(input_size))\n",
        "\n",
        "    # Preparation Network\n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_3x3')(input_S)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_4x4')(input_S)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_5x5')(input_S)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x = concatenate([input_C, x])\n",
        "    \n",
        "    # Hiding network\n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid4_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid4_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid5_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    output_Cprime = Conv2D(3, (3, 3), strides = (1, 1), padding='same', activation='relu', name='output_C')(x)\n",
        "    \n",
        "    return Model(inputs=[input_S, input_C],\n",
        "                 outputs=output_Cprime,\n",
        "                 name = 'Encoder')\n",
        "\n",
        "# Returns the decoder as a Keras model, composed by the Reveal Network\n",
        "def make_decoder(input_size, fixed=False):\n",
        "    \n",
        "    # Reveal network\n",
        "    reveal_input = Input(shape=(input_size))\n",
        "    \n",
        "    # Adding Gaussian noise with 0.01 standard deviation.\n",
        "    input_with_noise = GaussianNoise(0.01, name='output_C_noise')(reveal_input)\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_3x3')(input_with_noise)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_4x4')(input_with_noise)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_5x5')(input_with_noise)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev4_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev4_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev5_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    output_Sprime = Conv2D(3, (3, 3), strides = (1, 1), padding='same', activation='relu', name='output_S')(x)\n",
        "    \n",
        "    if not fixed:\n",
        "        return Model(inputs=reveal_input,\n",
        "                     outputs=output_Sprime,\n",
        "                     name = 'Decoder')\n",
        "    else:\n",
        "        return Container(inputs=reveal_input,\n",
        "                         outputs=output_Sprime,\n",
        "                         name = 'DecoderFixed')\n",
        "\n",
        "# Full model.\n",
        "def make_model(input_size):\n",
        "    input_S = Input(shape=(input_size))\n",
        "    input_C= Input(shape=(input_size))\n",
        "    \n",
        "    encoder = make_encoder(input_size)\n",
        "    \n",
        "    decoder = make_decoder(input_size)\n",
        "    decoder.compile(optimizer='adam', loss=rev_loss)\n",
        "    decoder.trainable = False\n",
        "    \n",
        "    output_Cprime = encoder([input_S, input_C])\n",
        "    output_Sprime = decoder(output_Cprime)\n",
        "\n",
        "    autoencoder = Model(inputs=[input_S, input_C],\n",
        "                        outputs=concatenate([output_Sprime, output_Cprime]))\n",
        "    autoencoder.compile(optimizer='adam', loss=full_loss)\n",
        "    \n",
        "    return encoder, decoder, autoencoder\n",
        "\n",
        "encoder_model, reveal_model, autoencoder_model = make_model(input_S.shape[1:])\n",
        "\n",
        "def lr_schedule(epoch_idx):\n",
        "    if epoch_idx < 200:\n",
        "        return 0.001\n",
        "    elif epoch_idx < 400:\n",
        "        return 0.0003\n",
        "    elif epoch_idx < 600:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00003\n",
        "\n",
        "\n",
        "autoencoder_model.load_weights('/content/gdrive/MyDrive/imagestegan/model/singlemodel500.hdf5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgAWBnxKaim3"
      },
      "source": [
        "decoded = autoencoder_model.predict([input_S, input_C])\n",
        "decoded_S, decoded_C = decoded[...,0:3], decoded[...,3:6]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsgHpoAgGACC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb619d33-f200-4326-85c9-4d25f5f3f331"
      },
      "source": [
        "# Variable used to weight the losses of the secret and cover images (See paper for more details)\n",
        "beta = 1.0\n",
        "    \n",
        "# Loss for reveal network\n",
        "def rev_loss(s_true, s_pred):\n",
        "    # Loss for reveal network is: beta * |S-S'|\n",
        "    return beta * K.sum(K.square(s_true - s_pred))\n",
        "\n",
        "# Loss for the full model, used for preparation and hidding networks\n",
        "def full_loss(y_true, y_pred):\n",
        "    # Loss for the full model is: |C-C'| + beta * |S-S'|\n",
        "    s_true, c_true = y_true[:,:,:,0:3], y_true[:,:,:,3:6]\n",
        "    s_pred, c_pred = y_pred[:,:,:,0:3], y_pred[:,:,:,3:6]\n",
        "    \n",
        "    s_loss = beta * K.sum(K.square(s_true - s_pred))\n",
        "    c_loss = K.sum(K.square(c_true - c_pred))\n",
        "    \n",
        "    return s_loss + c_loss\n",
        "\n",
        "\n",
        "# Returns the encoder as a Keras model, composed by Preparation and Hiding Networks.\n",
        "def make_encoder(input_size):\n",
        "    input_S = Input(shape=(input_size))\n",
        "    input_C= Input(shape=(input_size))\n",
        "\n",
        "    # Preparation Network\n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_3x3')(input_S)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_4x4')(input_S)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_prep0_5x5')(input_S)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_prep1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x = concatenate([input_C, x])\n",
        "    \n",
        "    # Hiding network\n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid0_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid2_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid3_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_hid4_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_hid4_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_hid5_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    output_Cprime = Conv2D(3, (3, 3), strides = (1, 1), padding='same', activation='relu', name='output_C')(x)\n",
        "    \n",
        "    return Model(inputs=[input_S, input_C],\n",
        "                 outputs=output_Cprime,\n",
        "                 name = 'Encoder')\n",
        "\n",
        "# Returns the decoder as a Keras model, composed by the Reveal Network\n",
        "def make_decoder(input_size, fixed=False):\n",
        "    \n",
        "    # Reveal network\n",
        "    reveal_input = Input(shape=(input_size))\n",
        "    \n",
        "    # Adding Gaussian noise with 0.01 standard deviation.\n",
        "    input_with_noise = GaussianNoise(0.01, name='output_C_noise')(reveal_input)\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_3x3')(input_with_noise)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_4x4')(input_with_noise)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev0_5x5')(input_with_noise)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev1_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev2_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev3_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    x3 = Conv2D(50, (3, 3), strides = (1, 1), padding='same', activation='relu', name='conv_rev4_3x3')(x)\n",
        "    x4 = Conv2D(10, (4, 4), strides = (1, 1), padding='same', activation='relu', name='conv_rev4_4x4')(x)\n",
        "    x5 = Conv2D(5, (5, 5), strides = (1, 1), padding='same', activation='relu', name='conv_rev5_5x5')(x)\n",
        "    x = concatenate([x3, x4, x5])\n",
        "    \n",
        "    output_Sprime = Conv2D(3, (3, 3), strides = (1, 1), padding='same', activation='relu', name='output_S')(x)\n",
        "    \n",
        "    if not fixed:\n",
        "        return Model(inputs=reveal_input,\n",
        "                     outputs=output_Sprime,\n",
        "                     name = 'Decoder')\n",
        "    else:\n",
        "        return Container(inputs=reveal_input,\n",
        "                         outputs=output_Sprime,\n",
        "                         name = 'DecoderFixed')\n",
        "\n",
        "# Full model.\n",
        "def make_model(input_size):\n",
        "    input_S = Input(shape=(input_size))\n",
        "    input_C= Input(shape=(input_size))\n",
        "    \n",
        "    encoder = make_encoder(input_size)\n",
        "    \n",
        "    decoder = make_decoder(input_size)\n",
        "    decoder.compile(optimizer='adam', loss=rev_loss)\n",
        "    decoder.trainable = False\n",
        "    \n",
        "    output_Cprime = encoder([input_S, input_C])\n",
        "    output_Sprime = decoder(output_Cprime)\n",
        "\n",
        "    autoencoder = Model(inputs=[input_S, input_C],\n",
        "                        outputs=concatenate([output_Sprime, output_Cprime]))\n",
        "    autoencoder.compile(optimizer='adam', loss=full_loss)\n",
        "    \n",
        "    return encoder, decoder, autoencoder\n",
        "\n",
        "encoder_model, reveal_model, autoencoder_model = make_model(input_S.shape[1:])\n",
        "\n",
        "def lr_schedule(epoch_idx):\n",
        "    if epoch_idx < 200:\n",
        "        return 0.001\n",
        "    elif epoch_idx < 400:\n",
        "        return 0.0003\n",
        "    elif epoch_idx < 600:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00003\n",
        "    \n",
        "NB_EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "m = input_S.shape[0]\n",
        "loss_history = []\n",
        "for epoch in range(NB_EPOCHS):\n",
        "    np.random.shuffle(input_S)\n",
        "    np.random.shuffle(input_C)\n",
        "    \n",
        "    t = tqdm(range(0, input_S.shape[0], BATCH_SIZE),mininterval=0)\n",
        "    ae_loss = []\n",
        "    rev_loss = []\n",
        "    for idx in t:\n",
        "        \n",
        "        batch_S = decoded_S[idx:min(idx + BATCH_SIZE, m)]\n",
        "        batch_C = input_C[idx:min(idx + BATCH_SIZE, m)]\n",
        "        \n",
        "        C_prime = encoder_model.predict([batch_S, batch_C])\n",
        "        \n",
        "        ae_loss.append(autoencoder_model.train_on_batch(x=[batch_S, batch_C],\n",
        "                                                   y=np.concatenate((batch_S, batch_C),axis=3)))\n",
        "        rev_loss.append(reveal_model.train_on_batch(x=C_prime,\n",
        "                                              y=batch_S))\n",
        "        \n",
        "        # Update learning rate\n",
        "        K.set_value(autoencoder_model.optimizer.lr, lr_schedule(epoch))\n",
        "        K.set_value(reveal_model.optimizer.lr, lr_schedule(epoch))\n",
        "        \n",
        "        t.set_description('Epoch {} | Batch: {:3} of {}. Loss AE {:10.2f} | Loss Rev {:10.2f}'.format(epoch + 1, idx, m, np.mean(ae_loss), np.mean(rev_loss)))\n",
        "    loss_history.append(np.mean(ae_loss))\n",
        "    \n",
        "    \n",
        "plt.plot(loss_history)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "\n",
        "autoencoder_model.save_weights('/content/gdrive/MyDrive/imagestegan/model/stackedmodel.hdf5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 | Batch: 960 of 1000. Loss AE  279253.07 | Loss Rev  172832.20: 100%|██████████| 16/16 [00:39<00:00,  2.46s/it]\n",
            "Epoch 2 | Batch: 960 of 1000. Loss AE  134829.32 | Loss Rev   45549.13: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 3 | Batch: 960 of 1000. Loss AE  107325.51 | Loss Rev   53738.40: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 4 | Batch: 960 of 1000. Loss AE   78836.19 | Loss Rev   39262.59: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 5 | Batch: 960 of 1000. Loss AE   55828.13 | Loss Rev   34154.39: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 6 | Batch: 960 of 1000. Loss AE   47836.75 | Loss Rev   28936.49: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 7 | Batch: 960 of 1000. Loss AE   40968.94 | Loss Rev   24725.47: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 8 | Batch: 960 of 1000. Loss AE   44255.25 | Loss Rev   24823.91: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 9 | Batch: 960 of 1000. Loss AE   27007.58 | Loss Rev   11145.21: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 10 | Batch: 960 of 1000. Loss AE   19549.70 | Loss Rev    7807.21: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 11 | Batch: 960 of 1000. Loss AE   17921.91 | Loss Rev    8127.32: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 12 | Batch: 960 of 1000. Loss AE   13420.37 | Loss Rev    5126.35: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 13 | Batch: 960 of 1000. Loss AE   16045.64 | Loss Rev    8394.69: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 14 | Batch: 960 of 1000. Loss AE   11849.69 | Loss Rev    4422.64: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 15 | Batch: 960 of 1000. Loss AE    9495.25 | Loss Rev    3101.97: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 16 | Batch: 960 of 1000. Loss AE   18482.79 | Loss Rev   11294.11: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 17 | Batch: 960 of 1000. Loss AE   16674.98 | Loss Rev    8533.43: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 18 | Batch: 960 of 1000. Loss AE    9632.21 | Loss Rev    3164.85: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 19 | Batch: 960 of 1000. Loss AE    7803.39 | Loss Rev    2339.49: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 20 | Batch: 960 of 1000. Loss AE    7640.04 | Loss Rev    2830.69: 100%|██████████| 16/16 [00:29<00:00,  1.85s/it]\n",
            "Epoch 21 | Batch: 960 of 1000. Loss AE    8130.02 | Loss Rev    3237.34: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 22 | Batch: 960 of 1000. Loss AE    6611.66 | Loss Rev    2184.33: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 23 | Batch: 960 of 1000. Loss AE    6941.54 | Loss Rev    2732.62: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 24 | Batch: 960 of 1000. Loss AE    6798.32 | Loss Rev    2736.03: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 25 | Batch: 960 of 1000. Loss AE    6410.60 | Loss Rev    2437.96: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 26 | Batch: 960 of 1000. Loss AE    6439.60 | Loss Rev    2593.54: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 27 | Batch: 960 of 1000. Loss AE    5976.52 | Loss Rev    2433.46: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 28 | Batch: 960 of 1000. Loss AE    6772.42 | Loss Rev    3006.62: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 29 | Batch: 960 of 1000. Loss AE    6052.39 | Loss Rev    2557.39: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 30 | Batch: 960 of 1000. Loss AE    5541.53 | Loss Rev    2159.73: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 31 | Batch: 960 of 1000. Loss AE    5707.39 | Loss Rev    2438.97: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 32 | Batch: 960 of 1000. Loss AE    8123.86 | Loss Rev    4389.14: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 33 | Batch: 960 of 1000. Loss AE    5887.90 | Loss Rev    2166.49: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 34 | Batch: 960 of 1000. Loss AE    4605.23 | Loss Rev    1488.46: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 35 | Batch: 960 of 1000. Loss AE    6211.05 | Loss Rev    2996.32: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 36 | Batch: 960 of 1000. Loss AE    5550.03 | Loss Rev    2378.18: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 37 | Batch: 960 of 1000. Loss AE    4901.46 | Loss Rev    1994.56: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 38 | Batch: 960 of 1000. Loss AE    4107.02 | Loss Rev    1374.55: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 39 | Batch: 960 of 1000. Loss AE    5057.07 | Loss Rev    2294.76: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 40 | Batch: 960 of 1000. Loss AE    6239.50 | Loss Rev    3397.57: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 41 | Batch: 960 of 1000. Loss AE    4375.29 | Loss Rev    1629.21: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 42 | Batch: 960 of 1000. Loss AE    3752.73 | Loss Rev    1212.01: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 43 | Batch: 960 of 1000. Loss AE   15745.10 | Loss Rev    9826.12: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 44 | Batch: 960 of 1000. Loss AE    9845.92 | Loss Rev    3640.53: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 45 | Batch: 960 of 1000. Loss AE    5875.18 | Loss Rev    2008.50: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 46 | Batch: 960 of 1000. Loss AE    5094.95 | Loss Rev    2046.14: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 47 | Batch: 960 of 1000. Loss AE    7597.37 | Loss Rev    4486.54: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 48 | Batch: 960 of 1000. Loss AE    4784.41 | Loss Rev    1814.24: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 49 | Batch: 960 of 1000. Loss AE    3949.25 | Loss Rev    1333.34: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 50 | Batch: 960 of 1000. Loss AE    3810.66 | Loss Rev    1358.78: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 51 | Batch: 960 of 1000. Loss AE    8618.68 | Loss Rev    5056.23: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 52 | Batch: 960 of 1000. Loss AE    5517.05 | Loss Rev    2341.89: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 53 | Batch: 960 of 1000. Loss AE    3996.07 | Loss Rev    1415.27: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 54 | Batch: 960 of 1000. Loss AE    3822.58 | Loss Rev    1464.74: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 55 | Batch: 960 of 1000. Loss AE    3697.21 | Loss Rev    1399.51: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 56 | Batch: 960 of 1000. Loss AE    3608.28 | Loss Rev    1380.97: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 57 | Batch: 960 of 1000. Loss AE    3727.60 | Loss Rev    1526.60: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 58 | Batch: 960 of 1000. Loss AE    3868.31 | Loss Rev    1637.44: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 59 | Batch: 960 of 1000. Loss AE    3458.80 | Loss Rev    1323.86: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 60 | Batch: 960 of 1000. Loss AE    3469.68 | Loss Rev    1355.41: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 61 | Batch: 960 of 1000. Loss AE    3401.72 | Loss Rev    1351.38: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 62 | Batch: 960 of 1000. Loss AE    5653.35 | Loss Rev    3362.56: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 63 | Batch: 960 of 1000. Loss AE    5578.00 | Loss Rev    2802.14: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 64 | Batch: 960 of 1000. Loss AE    4610.68 | Loss Rev    2258.68: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 65 | Batch: 960 of 1000. Loss AE    3887.80 | Loss Rev    1620.63: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 66 | Batch: 960 of 1000. Loss AE    3159.41 | Loss Rev    1106.11: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 67 | Batch: 960 of 1000. Loss AE    2999.84 | Loss Rev    1069.55: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 68 | Batch: 960 of 1000. Loss AE    4166.35 | Loss Rev    2050.96: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 69 | Batch: 960 of 1000. Loss AE    3319.27 | Loss Rev    1277.65: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 70 | Batch: 960 of 1000. Loss AE    3173.11 | Loss Rev    1250.90: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 71 | Batch: 960 of 1000. Loss AE    2816.22 | Loss Rev     954.82: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 72 | Batch: 960 of 1000. Loss AE   92936.86 | Loss Rev   78067.54: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 73 | Batch: 960 of 1000. Loss AE   74473.14 | Loss Rev   42373.27: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 74 | Batch: 960 of 1000. Loss AE   54994.42 | Loss Rev   34589.74: 100%|██████████| 16/16 [00:26<00:00,  1.69s/it]\n",
            "Epoch 75 | Batch: 960 of 1000. Loss AE   21483.06 | Loss Rev    9289.48: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 76 | Batch: 960 of 1000. Loss AE   14313.89 | Loss Rev    5545.02: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 77 | Batch: 960 of 1000. Loss AE   15228.77 | Loss Rev    7892.24: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 78 | Batch: 960 of 1000. Loss AE   11287.29 | Loss Rev    5118.55: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 79 | Batch: 960 of 1000. Loss AE    8693.70 | Loss Rev    3831.11: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 80 | Batch: 960 of 1000. Loss AE    6883.17 | Loss Rev    2699.14: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 81 | Batch: 960 of 1000. Loss AE    6437.42 | Loss Rev    2623.49: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 82 | Batch: 960 of 1000. Loss AE    6101.11 | Loss Rev    2517.46: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 83 | Batch: 960 of 1000. Loss AE    5512.40 | Loss Rev    2064.12: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 84 | Batch: 960 of 1000. Loss AE    7648.60 | Loss Rev    3927.05: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 85 | Batch: 960 of 1000. Loss AE    5966.32 | Loss Rev    2377.77: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 86 | Batch: 960 of 1000. Loss AE    4759.52 | Loss Rev    1711.54: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 87 | Batch: 960 of 1000. Loss AE    6558.76 | Loss Rev    3255.03: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 88 | Batch: 960 of 1000. Loss AE    5211.14 | Loss Rev    2004.32: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 89 | Batch: 960 of 1000. Loss AE    4535.10 | Loss Rev    1685.15: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 90 | Batch: 960 of 1000. Loss AE    4568.13 | Loss Rev    1879.28: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 91 | Batch: 960 of 1000. Loss AE    4992.58 | Loss Rev    2142.58: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 92 | Batch: 960 of 1000. Loss AE    4303.02 | Loss Rev    1625.99: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 93 | Batch: 960 of 1000. Loss AE    3815.84 | Loss Rev    1301.95: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 94 | Batch: 960 of 1000. Loss AE   23384.24 | Loss Rev   16256.14: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 95 | Batch: 960 of 1000. Loss AE   13661.45 | Loss Rev    4950.38: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 96 | Batch: 960 of 1000. Loss AE    6772.22 | Loss Rev    2524.65: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 97 | Batch: 960 of 1000. Loss AE    5163.30 | Loss Rev    2023.87: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 98 | Batch: 960 of 1000. Loss AE    4454.55 | Loss Rev    1699.53: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 99 | Batch: 960 of 1000. Loss AE    4114.81 | Loss Rev    1532.48: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 100 | Batch: 960 of 1000. Loss AE    6172.30 | Loss Rev    3216.92: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 101 | Batch: 960 of 1000. Loss AE    4794.16 | Loss Rev    1871.35: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 102 | Batch: 960 of 1000. Loss AE    3966.24 | Loss Rev    1422.22: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 103 | Batch: 960 of 1000. Loss AE    3752.27 | Loss Rev    1352.85: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 104 | Batch: 960 of 1000. Loss AE    3648.45 | Loss Rev    1317.50: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 105 | Batch: 960 of 1000. Loss AE    3730.73 | Loss Rev    1415.32: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 106 | Batch: 960 of 1000. Loss AE    3565.57 | Loss Rev    1293.59: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 107 | Batch: 960 of 1000. Loss AE    3418.40 | Loss Rev    1201.65: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 108 | Batch: 960 of 1000. Loss AE    8465.80 | Loss Rev    5115.42: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 109 | Batch: 960 of 1000. Loss AE    5552.44 | Loss Rev    2388.90: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 110 | Batch: 960 of 1000. Loss AE    4007.49 | Loss Rev    1463.28: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 111 | Batch: 960 of 1000. Loss AE    3654.73 | Loss Rev    1373.89: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 112 | Batch: 960 of 1000. Loss AE    3345.16 | Loss Rev    1179.25: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 113 | Batch: 960 of 1000. Loss AE    3238.95 | Loss Rev    1135.97: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 114 | Batch: 960 of 1000. Loss AE    3152.49 | Loss Rev    1091.86: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 115 | Batch: 960 of 1000. Loss AE    3564.17 | Loss Rev    1448.94: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 116 | Batch: 960 of 1000. Loss AE    3073.27 | Loss Rev     996.11: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 117 | Batch: 960 of 1000. Loss AE    3535.26 | Loss Rev    1453.00: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 118 | Batch: 960 of 1000. Loss AE    3371.12 | Loss Rev    1278.43: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 119 | Batch: 960 of 1000. Loss AE    3087.45 | Loss Rev    1065.75: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 120 | Batch: 960 of 1000. Loss AE    3155.37 | Loss Rev    1147.93: 100%|██████████| 16/16 [00:26<00:00,  1.69s/it]\n",
            "Epoch 121 | Batch: 960 of 1000. Loss AE    3003.39 | Loss Rev    1037.20: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 122 | Batch: 960 of 1000. Loss AE    3222.44 | Loss Rev    1244.18: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 123 | Batch: 960 of 1000. Loss AE    2994.76 | Loss Rev    1053.16: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 124 | Batch: 960 of 1000. Loss AE    2877.85 | Loss Rev     974.31: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 125 | Batch: 960 of 1000. Loss AE    5715.52 | Loss Rev    3290.13: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 126 | Batch: 960 of 1000. Loss AE    3848.53 | Loss Rev    1529.57: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 127 | Batch: 960 of 1000. Loss AE    3084.20 | Loss Rev    1075.08: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 128 | Batch: 960 of 1000. Loss AE    2830.68 | Loss Rev     968.27: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 129 | Batch: 960 of 1000. Loss AE    2833.32 | Loss Rev    1008.54: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 130 | Batch: 960 of 1000. Loss AE    2703.30 | Loss Rev     905.57: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 131 | Batch: 960 of 1000. Loss AE    3998.84 | Loss Rev    1977.31: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 132 | Batch: 960 of 1000. Loss AE    2890.95 | Loss Rev     949.82: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 133 | Batch: 960 of 1000. Loss AE    2634.13 | Loss Rev     839.71: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 134 | Batch: 960 of 1000. Loss AE    2728.70 | Loss Rev     959.78: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 135 | Batch: 960 of 1000. Loss AE    2655.84 | Loss Rev     889.54: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 136 | Batch: 960 of 1000. Loss AE   30899.09 | Loss Rev   25625.80: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 137 | Batch: 960 of 1000. Loss AE  134909.30 | Loss Rev  109695.01: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 138 | Batch: 960 of 1000. Loss AE   27205.13 | Loss Rev   12811.59: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 139 | Batch: 960 of 1000. Loss AE   15757.62 | Loss Rev    6442.69: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 140 | Batch: 960 of 1000. Loss AE   11064.96 | Loss Rev    4113.67: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 141 | Batch: 960 of 1000. Loss AE    9600.61 | Loss Rev    4212.82: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 142 | Batch: 960 of 1000. Loss AE    7455.44 | Loss Rev    3224.41: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 143 | Batch: 960 of 1000. Loss AE   14113.85 | Loss Rev    6835.20: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 144 | Batch: 960 of 1000. Loss AE    8396.33 | Loss Rev    3335.43: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 145 | Batch: 960 of 1000. Loss AE    6292.42 | Loss Rev    2807.84: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 146 | Batch: 960 of 1000. Loss AE    5435.60 | Loss Rev    2427.98: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 147 | Batch: 960 of 1000. Loss AE    5885.76 | Loss Rev    2893.86: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 148 | Batch: 960 of 1000. Loss AE    5251.33 | Loss Rev    2330.41: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 149 | Batch: 960 of 1000. Loss AE    5196.79 | Loss Rev    2516.13: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 150 | Batch: 960 of 1000. Loss AE    7362.85 | Loss Rev    3807.27: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 151 | Batch: 960 of 1000. Loss AE    5266.40 | Loss Rev    2327.55: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 152 | Batch: 960 of 1000. Loss AE    4426.12 | Loss Rev    1918.40: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 153 | Batch: 960 of 1000. Loss AE    4802.63 | Loss Rev    2252.47: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 154 | Batch: 960 of 1000. Loss AE    4713.10 | Loss Rev    2184.24: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 155 | Batch: 960 of 1000. Loss AE    4756.90 | Loss Rev    2211.61: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 156 | Batch: 960 of 1000. Loss AE    4349.75 | Loss Rev    1921.97: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 157 | Batch: 960 of 1000. Loss AE    3855.98 | Loss Rev    1538.97: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 158 | Batch: 960 of 1000. Loss AE    4164.61 | Loss Rev    1883.17: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 159 | Batch: 960 of 1000. Loss AE    3991.07 | Loss Rev    1596.65: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 160 | Batch: 960 of 1000. Loss AE    4745.95 | Loss Rev    2265.12: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 161 | Batch: 960 of 1000. Loss AE    4024.98 | Loss Rev    1593.44: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 162 | Batch: 960 of 1000. Loss AE    3480.43 | Loss Rev    1250.43: 100%|██████████| 16/16 [00:26<00:00,  1.69s/it]\n",
            "Epoch 163 | Batch: 960 of 1000. Loss AE    3357.21 | Loss Rev    1249.89: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 164 | Batch: 960 of 1000. Loss AE    4203.35 | Loss Rev    1974.38: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 165 | Batch: 960 of 1000. Loss AE    3980.32 | Loss Rev    1731.80: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 166 | Batch: 960 of 1000. Loss AE    3229.14 | Loss Rev    1150.82: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 167 | Batch: 960 of 1000. Loss AE    5515.34 | Loss Rev    2919.28: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 168 | Batch: 960 of 1000. Loss AE    4227.59 | Loss Rev    1819.93: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 169 | Batch: 960 of 1000. Loss AE    3423.83 | Loss Rev    1334.42: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 170 | Batch: 960 of 1000. Loss AE    3531.31 | Loss Rev    1525.05: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 171 | Batch: 960 of 1000. Loss AE    3286.04 | Loss Rev    1239.29: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 172 | Batch: 960 of 1000. Loss AE    3163.05 | Loss Rev    1207.89: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 173 | Batch: 960 of 1000. Loss AE    3532.11 | Loss Rev    1486.52: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 174 | Batch: 960 of 1000. Loss AE    3249.76 | Loss Rev    1239.00: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 175 | Batch: 960 of 1000. Loss AE    3190.70 | Loss Rev    1267.71: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 176 | Batch: 960 of 1000. Loss AE    4533.18 | Loss Rev    2267.44: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 177 | Batch: 960 of 1000. Loss AE    3271.98 | Loss Rev    1180.40: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 178 | Batch: 960 of 1000. Loss AE    2897.15 | Loss Rev    1026.16: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 179 | Batch: 960 of 1000. Loss AE    3260.92 | Loss Rev    1362.17: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 180 | Batch: 960 of 1000. Loss AE    3013.91 | Loss Rev    1135.58: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 181 | Batch: 960 of 1000. Loss AE    2925.94 | Loss Rev    1110.63: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 182 | Batch: 960 of 1000. Loss AE    3258.23 | Loss Rev    1358.67: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 183 | Batch: 960 of 1000. Loss AE    2917.86 | Loss Rev    1065.33: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 184 | Batch: 960 of 1000. Loss AE    2651.65 | Loss Rev     889.21: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 185 | Batch: 960 of 1000. Loss AE    2520.69 | Loss Rev     819.76: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 186 | Batch: 960 of 1000. Loss AE    2776.73 | Loss Rev    1081.30: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 187 | Batch: 960 of 1000. Loss AE    2831.39 | Loss Rev    1032.47: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 188 | Batch: 960 of 1000. Loss AE    3024.99 | Loss Rev    1248.10: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 189 | Batch: 960 of 1000. Loss AE    2715.29 | Loss Rev     969.55: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 190 | Batch: 960 of 1000. Loss AE    2748.71 | Loss Rev    1015.17: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 191 | Batch: 960 of 1000. Loss AE    2445.55 | Loss Rev     783.51: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 192 | Batch: 960 of 1000. Loss AE    2405.23 | Loss Rev     792.09: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 193 | Batch: 960 of 1000. Loss AE    8431.35 | Loss Rev    4705.49: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 194 | Batch: 960 of 1000. Loss AE    4709.33 | Loss Rev    2172.68: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 195 | Batch: 960 of 1000. Loss AE    3463.66 | Loss Rev    1505.14: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 196 | Batch: 960 of 1000. Loss AE    4811.25 | Loss Rev    2667.36: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 197 | Batch: 960 of 1000. Loss AE    3613.88 | Loss Rev    1646.69: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 198 | Batch: 960 of 1000. Loss AE    2805.53 | Loss Rev    1072.78: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 199 | Batch: 960 of 1000. Loss AE    2721.47 | Loss Rev    1033.22: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 200 | Batch: 960 of 1000. Loss AE    2927.31 | Loss Rev    1237.23: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 201 | Batch: 960 of 1000. Loss AE    2707.91 | Loss Rev     982.33: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 202 | Batch: 960 of 1000. Loss AE    2435.94 | Loss Rev     781.71: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 203 | Batch: 960 of 1000. Loss AE    2385.84 | Loss Rev     761.14: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 204 | Batch: 960 of 1000. Loss AE    2358.47 | Loss Rev     754.84: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 205 | Batch: 960 of 1000. Loss AE    2342.27 | Loss Rev     751.26: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 206 | Batch: 960 of 1000. Loss AE    2328.97 | Loss Rev     746.17: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 207 | Batch: 960 of 1000. Loss AE    2319.50 | Loss Rev     740.87: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 208 | Batch: 960 of 1000. Loss AE    2306.40 | Loss Rev     737.64: 100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
            "Epoch 209 | Batch: 960 of 1000. Loss AE    2293.91 | Loss Rev     727.94: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 210 | Batch: 960 of 1000. Loss AE    2289.62 | Loss Rev     728.00: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 211 | Batch: 960 of 1000. Loss AE    2278.72 | Loss Rev     723.67: 100%|██████████| 16/16 [00:26<00:00,  1.69s/it]\n",
            "Epoch 212 | Batch: 960 of 1000. Loss AE    2270.41 | Loss Rev     715.92: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 213 | Batch: 960 of 1000. Loss AE    2258.66 | Loss Rev     714.92: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 214 | Batch: 960 of 1000. Loss AE    2254.57 | Loss Rev     712.40: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 215 | Batch: 960 of 1000. Loss AE    2242.17 | Loss Rev     704.30: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 216 | Batch: 960 of 1000. Loss AE    2230.20 | Loss Rev     700.84: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 217 | Batch: 960 of 1000. Loss AE    2227.92 | Loss Rev     696.49: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 218 | Batch: 960 of 1000. Loss AE    2224.75 | Loss Rev     691.12: 100%|██████████| 16/16 [00:29<00:00,  1.86s/it]\n",
            "Epoch 219 | Batch: 960 of 1000. Loss AE    2210.98 | Loss Rev     690.29: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 220 | Batch: 960 of 1000. Loss AE    2202.99 | Loss Rev     688.41: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 221 | Batch: 960 of 1000. Loss AE    2196.46 | Loss Rev     686.13: 100%|██████████| 16/16 [00:29<00:00,  1.84s/it]\n",
            "Epoch 222 | Batch: 960 of 1000. Loss AE    2194.48 | Loss Rev     684.77: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 223 | Batch: 960 of 1000. Loss AE    2183.68 | Loss Rev     678.99: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 224 | Batch: 960 of 1000. Loss AE    2175.58 | Loss Rev     672.28: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 225 | Batch: 960 of 1000. Loss AE    2171.23 | Loss Rev     670.60: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 226 | Batch: 960 of 1000. Loss AE    2162.61 | Loss Rev     665.43: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 227 | Batch: 960 of 1000. Loss AE    2154.26 | Loss Rev     666.28: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 228 | Batch: 960 of 1000. Loss AE    2152.21 | Loss Rev     661.83: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 229 | Batch: 960 of 1000. Loss AE    2146.44 | Loss Rev     661.64: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 230 | Batch: 960 of 1000. Loss AE    2138.62 | Loss Rev     659.71: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 231 | Batch: 960 of 1000. Loss AE    2132.51 | Loss Rev     653.20: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 232 | Batch: 960 of 1000. Loss AE    2128.01 | Loss Rev     652.70: 100%|██████████| 16/16 [00:29<00:00,  1.84s/it]\n",
            "Epoch 233 | Batch: 960 of 1000. Loss AE    2119.71 | Loss Rev     652.02: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 234 | Batch: 960 of 1000. Loss AE    2114.86 | Loss Rev     647.92: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 235 | Batch: 960 of 1000. Loss AE    2107.29 | Loss Rev     646.20: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 236 | Batch: 960 of 1000. Loss AE    2105.60 | Loss Rev     644.88: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 237 | Batch: 960 of 1000. Loss AE    2100.94 | Loss Rev     640.74: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 238 | Batch: 960 of 1000. Loss AE    2091.37 | Loss Rev     641.71: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 239 | Batch: 960 of 1000. Loss AE    2093.63 | Loss Rev     642.50: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 240 | Batch: 960 of 1000. Loss AE    2086.05 | Loss Rev     632.77: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 241 | Batch: 960 of 1000. Loss AE    2077.16 | Loss Rev     631.52: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 242 | Batch: 960 of 1000. Loss AE    2072.28 | Loss Rev     629.92: 100%|██████████| 16/16 [00:29<00:00,  1.85s/it]\n",
            "Epoch 243 | Batch: 960 of 1000. Loss AE    2069.40 | Loss Rev     631.57: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 244 | Batch: 960 of 1000. Loss AE    2065.87 | Loss Rev     628.57: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 245 | Batch: 960 of 1000. Loss AE    2053.97 | Loss Rev     625.83: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 246 | Batch: 960 of 1000. Loss AE    2051.20 | Loss Rev     623.37: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 247 | Batch: 960 of 1000. Loss AE    2046.62 | Loss Rev     620.62: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 248 | Batch: 960 of 1000. Loss AE    2073.64 | Loss Rev     650.63: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 249 | Batch: 960 of 1000. Loss AE    2113.69 | Loss Rev     680.31: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 250 | Batch: 960 of 1000. Loss AE    2134.44 | Loss Rev     690.66: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 251 | Batch: 960 of 1000. Loss AE    2048.74 | Loss Rev     616.32: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 252 | Batch: 960 of 1000. Loss AE    2059.15 | Loss Rev     637.92: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 253 | Batch: 960 of 1000. Loss AE    2036.59 | Loss Rev     619.67: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 254 | Batch: 960 of 1000. Loss AE    2042.95 | Loss Rev     628.64: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 255 | Batch: 960 of 1000. Loss AE    2022.62 | Loss Rev     611.81: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 256 | Batch: 960 of 1000. Loss AE    2007.42 | Loss Rev     607.68: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 257 | Batch: 960 of 1000. Loss AE    2001.74 | Loss Rev     604.47: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 258 | Batch: 960 of 1000. Loss AE    2003.48 | Loss Rev     601.94: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 259 | Batch: 960 of 1000. Loss AE    1994.45 | Loss Rev     601.74: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 260 | Batch: 960 of 1000. Loss AE    1992.25 | Loss Rev     603.70: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 261 | Batch: 960 of 1000. Loss AE    1981.93 | Loss Rev     596.51: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 262 | Batch: 960 of 1000. Loss AE    1979.97 | Loss Rev     597.52: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 263 | Batch: 960 of 1000. Loss AE    1982.56 | Loss Rev     597.22: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 264 | Batch: 960 of 1000. Loss AE    1970.55 | Loss Rev     593.22: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 265 | Batch: 960 of 1000. Loss AE    1988.48 | Loss Rev     606.68: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 266 | Batch: 960 of 1000. Loss AE    1966.48 | Loss Rev     589.90: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 267 | Batch: 960 of 1000. Loss AE    1994.06 | Loss Rev     617.86: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 268 | Batch: 960 of 1000. Loss AE    2007.91 | Loss Rev     621.74: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 269 | Batch: 960 of 1000. Loss AE    2004.75 | Loss Rev     622.41: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 270 | Batch: 960 of 1000. Loss AE    1953.37 | Loss Rev     583.10: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 271 | Batch: 960 of 1000. Loss AE    1956.49 | Loss Rev     591.30: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 272 | Batch: 960 of 1000. Loss AE    1955.63 | Loss Rev     593.45: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 273 | Batch: 960 of 1000. Loss AE    1967.59 | Loss Rev     604.18: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 274 | Batch: 960 of 1000. Loss AE    1961.96 | Loss Rev     601.42: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 275 | Batch: 960 of 1000. Loss AE    1931.44 | Loss Rev     575.79: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 276 | Batch: 960 of 1000. Loss AE    1995.93 | Loss Rev     638.26: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 277 | Batch: 960 of 1000. Loss AE    2011.37 | Loss Rev     644.25: 100%|██████████| 16/16 [00:26<00:00,  1.69s/it]\n",
            "Epoch 278 | Batch: 960 of 1000. Loss AE    1945.05 | Loss Rev     584.78: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 279 | Batch: 960 of 1000. Loss AE    1960.50 | Loss Rev     603.40: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 280 | Batch: 960 of 1000. Loss AE    1941.48 | Loss Rev     588.13: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 281 | Batch: 960 of 1000. Loss AE    1906.99 | Loss Rev     564.11: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 282 | Batch: 960 of 1000. Loss AE    1936.83 | Loss Rev     595.53: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 283 | Batch: 960 of 1000. Loss AE    2016.16 | Loss Rev     651.35: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 284 | Batch: 960 of 1000. Loss AE    1919.39 | Loss Rev     570.39: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 285 | Batch: 960 of 1000. Loss AE    1907.12 | Loss Rev     567.68: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 286 | Batch: 960 of 1000. Loss AE    1991.22 | Loss Rev     646.51: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 287 | Batch: 960 of 1000. Loss AE    1919.19 | Loss Rev     578.31: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 288 | Batch: 960 of 1000. Loss AE    1905.16 | Loss Rev     572.56: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 289 | Batch: 960 of 1000. Loss AE    1934.76 | Loss Rev     600.08: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 290 | Batch: 960 of 1000. Loss AE    1894.02 | Loss Rev     567.62: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 291 | Batch: 960 of 1000. Loss AE    1978.16 | Loss Rev     640.70: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 292 | Batch: 960 of 1000. Loss AE    1939.78 | Loss Rev     604.90: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 293 | Batch: 960 of 1000. Loss AE    1864.54 | Loss Rev     548.36: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 294 | Batch: 960 of 1000. Loss AE    1885.90 | Loss Rev     568.89: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 295 | Batch: 960 of 1000. Loss AE    1924.79 | Loss Rev     599.10: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 296 | Batch: 960 of 1000. Loss AE    1875.15 | Loss Rev     559.88: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 297 | Batch: 960 of 1000. Loss AE    2202.71 | Loss Rev     842.41: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 298 | Batch: 960 of 1000. Loss AE    2000.23 | Loss Rev     624.84: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 299 | Batch: 960 of 1000. Loss AE    2104.12 | Loss Rev     753.51: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 300 | Batch: 960 of 1000. Loss AE    2079.29 | Loss Rev     692.68: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 301 | Batch: 960 of 1000. Loss AE    1877.22 | Loss Rev     538.24: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 302 | Batch: 960 of 1000. Loss AE    1855.54 | Loss Rev     544.04: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 303 | Batch: 960 of 1000. Loss AE    1848.00 | Loss Rev     550.24: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 304 | Batch: 960 of 1000. Loss AE    1837.68 | Loss Rev     545.17: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 305 | Batch: 960 of 1000. Loss AE    1835.09 | Loss Rev     546.64: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 306 | Batch: 960 of 1000. Loss AE    1846.27 | Loss Rev     556.00: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 307 | Batch: 960 of 1000. Loss AE    1875.87 | Loss Rev     578.06: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 308 | Batch: 960 of 1000. Loss AE    1830.86 | Loss Rev     538.92: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 309 | Batch: 960 of 1000. Loss AE    1825.05 | Loss Rev     542.11: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 310 | Batch: 960 of 1000. Loss AE    1836.00 | Loss Rev     560.19: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 311 | Batch: 960 of 1000. Loss AE    1951.57 | Loss Rev     647.65: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 312 | Batch: 960 of 1000. Loss AE    1860.08 | Loss Rev     559.85: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 313 | Batch: 960 of 1000. Loss AE    1853.90 | Loss Rev     565.21: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 314 | Batch: 960 of 1000. Loss AE    1814.81 | Loss Rev     534.83: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 315 | Batch: 960 of 1000. Loss AE    1877.10 | Loss Rev     589.83: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 316 | Batch: 960 of 1000. Loss AE    1845.62 | Loss Rev     568.05: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 317 | Batch: 960 of 1000. Loss AE    1911.98 | Loss Rev     616.14: 100%|██████████| 16/16 [00:29<00:00,  1.84s/it]\n",
            "Epoch 318 | Batch: 960 of 1000. Loss AE    1889.52 | Loss Rev     593.37: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 319 | Batch: 960 of 1000. Loss AE    1813.33 | Loss Rev     531.33: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 320 | Batch: 960 of 1000. Loss AE    1809.09 | Loss Rev     539.74: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 321 | Batch: 960 of 1000. Loss AE    1816.49 | Loss Rev     550.07: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 322 | Batch: 960 of 1000. Loss AE    1807.82 | Loss Rev     539.28: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 323 | Batch: 960 of 1000. Loss AE    1782.95 | Loss Rev     528.01: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 324 | Batch: 960 of 1000. Loss AE    2148.90 | Loss Rev     837.91: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 325 | Batch: 960 of 1000. Loss AE    2240.13 | Loss Rev     863.29: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 326 | Batch: 960 of 1000. Loss AE    1871.62 | Loss Rev     538.19: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 327 | Batch: 960 of 1000. Loss AE    1815.95 | Loss Rev     540.77: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 328 | Batch: 960 of 1000. Loss AE    1811.88 | Loss Rev     555.50: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 329 | Batch: 960 of 1000. Loss AE    2102.05 | Loss Rev     797.09: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 330 | Batch: 960 of 1000. Loss AE    2004.74 | Loss Rev     671.02: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 331 | Batch: 960 of 1000. Loss AE    1804.81 | Loss Rev     515.70: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 332 | Batch: 960 of 1000. Loss AE    1774.40 | Loss Rev     526.88: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 333 | Batch: 960 of 1000. Loss AE    1767.68 | Loss Rev     525.46: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 334 | Batch: 960 of 1000. Loss AE    1758.16 | Loss Rev     522.34: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 335 | Batch: 960 of 1000. Loss AE    1779.92 | Loss Rev     544.41: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 336 | Batch: 960 of 1000. Loss AE    1766.79 | Loss Rev     525.93: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 337 | Batch: 960 of 1000. Loss AE    1980.39 | Loss Rev     718.44: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 338 | Batch: 960 of 1000. Loss AE    1810.73 | Loss Rev     535.23: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 339 | Batch: 960 of 1000. Loss AE    1781.51 | Loss Rev     532.13: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 340 | Batch: 960 of 1000. Loss AE    1784.26 | Loss Rev     545.20: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 341 | Batch: 960 of 1000. Loss AE    1741.40 | Loss Rev     512.00: 100%|██████████| 16/16 [00:30<00:00,  1.89s/it]\n",
            "Epoch 342 | Batch: 960 of 1000. Loss AE    1746.68 | Loss Rev     524.12: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 343 | Batch: 960 of 1000. Loss AE    1759.41 | Loss Rev     538.49: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 344 | Batch: 960 of 1000. Loss AE    2078.77 | Loss Rev     804.32: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 345 | Batch: 960 of 1000. Loss AE    1839.04 | Loss Rev     552.92: 100%|██████████| 16/16 [00:29<00:00,  1.84s/it]\n",
            "Epoch 346 | Batch: 960 of 1000. Loss AE    1766.92 | Loss Rev     529.33: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 347 | Batch: 960 of 1000. Loss AE    1736.08 | Loss Rev     516.82: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 348 | Batch: 960 of 1000. Loss AE    1720.25 | Loss Rev     507.11: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 349 | Batch: 960 of 1000. Loss AE    1717.48 | Loss Rev     511.38: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 350 | Batch: 960 of 1000. Loss AE    1768.17 | Loss Rev     554.05: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 351 | Batch: 960 of 1000. Loss AE    1743.87 | Loss Rev     524.96: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 352 | Batch: 960 of 1000. Loss AE    1727.99 | Loss Rev     520.57: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 353 | Batch: 960 of 1000. Loss AE    1699.81 | Loss Rev     497.79: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 354 | Batch: 960 of 1000. Loss AE    1851.81 | Loss Rev     630.93: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 355 | Batch: 960 of 1000. Loss AE    1816.08 | Loss Rev     584.01: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 356 | Batch: 960 of 1000. Loss AE    1824.38 | Loss Rev     596.42: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 357 | Batch: 960 of 1000. Loss AE    2168.10 | Loss Rev     870.63: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 358 | Batch: 960 of 1000. Loss AE    1907.74 | Loss Rev     609.32: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 359 | Batch: 960 of 1000. Loss AE    1740.70 | Loss Rev     507.10: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 360 | Batch: 960 of 1000. Loss AE    1707.27 | Loss Rev     510.59: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 361 | Batch: 960 of 1000. Loss AE    1746.63 | Loss Rev     552.50: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 362 | Batch: 960 of 1000. Loss AE    1698.91 | Loss Rev     506.48: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 363 | Batch: 960 of 1000. Loss AE    1684.17 | Loss Rev     500.44: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 364 | Batch: 960 of 1000. Loss AE    1696.34 | Loss Rev     514.93: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 365 | Batch: 960 of 1000. Loss AE    1682.96 | Loss Rev     505.55: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 366 | Batch: 960 of 1000. Loss AE    1862.94 | Loss Rev     650.92: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 367 | Batch: 960 of 1000. Loss AE    1728.79 | Loss Rev     521.16: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 368 | Batch: 960 of 1000. Loss AE    1729.90 | Loss Rev     542.68: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 369 | Batch: 960 of 1000. Loss AE    1676.04 | Loss Rev     499.56: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 370 | Batch: 960 of 1000. Loss AE    1695.35 | Loss Rev     518.89: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 371 | Batch: 960 of 1000. Loss AE    1676.00 | Loss Rev     504.19: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 372 | Batch: 960 of 1000. Loss AE    1671.42 | Loss Rev     500.96: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 373 | Batch: 960 of 1000. Loss AE    2337.73 | Loss Rev    1052.08: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 374 | Batch: 960 of 1000. Loss AE    2178.80 | Loss Rev     837.52: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 375 | Batch: 960 of 1000. Loss AE    1952.25 | Loss Rev     660.89: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 376 | Batch: 960 of 1000. Loss AE    1765.65 | Loss Rev     535.25: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 377 | Batch: 960 of 1000. Loss AE    1685.36 | Loss Rev     500.57: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 378 | Batch: 960 of 1000. Loss AE    1661.33 | Loss Rev     501.35: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 379 | Batch: 960 of 1000. Loss AE    1651.74 | Loss Rev     496.27: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 380 | Batch: 960 of 1000. Loss AE    1647.46 | Loss Rev     489.54: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 381 | Batch: 960 of 1000. Loss AE    1644.97 | Loss Rev     497.60: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 382 | Batch: 960 of 1000. Loss AE    1650.12 | Loss Rev     500.62: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 383 | Batch: 960 of 1000. Loss AE    1662.26 | Loss Rev     508.94: 100%|██████████| 16/16 [00:26<00:00,  1.68s/it]\n",
            "Epoch 384 | Batch: 960 of 1000. Loss AE    1671.22 | Loss Rev     519.32: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 385 | Batch: 960 of 1000. Loss AE    1669.46 | Loss Rev     511.29: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 386 | Batch: 960 of 1000. Loss AE    1698.58 | Loss Rev     545.79: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 387 | Batch: 960 of 1000. Loss AE    1964.62 | Loss Rev     758.50: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 388 | Batch: 960 of 1000. Loss AE    1745.58 | Loss Rev     543.18: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 389 | Batch: 960 of 1000. Loss AE    1637.91 | Loss Rev     484.95: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 390 | Batch: 960 of 1000. Loss AE    1629.17 | Loss Rev     494.25: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 391 | Batch: 960 of 1000. Loss AE    1626.98 | Loss Rev     494.34: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 392 | Batch: 960 of 1000. Loss AE    1638.71 | Loss Rev     506.70: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 393 | Batch: 960 of 1000. Loss AE    1698.85 | Loss Rev     555.21: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 394 | Batch: 960 of 1000. Loss AE    1712.46 | Loss Rev     558.31: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 395 | Batch: 960 of 1000. Loss AE    1636.81 | Loss Rev     495.98: 100%|██████████| 16/16 [00:29<00:00,  1.84s/it]\n",
            "Epoch 396 | Batch: 960 of 1000. Loss AE    1630.06 | Loss Rev     501.19: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 397 | Batch: 960 of 1000. Loss AE    1719.46 | Loss Rev     577.20: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 398 | Batch: 960 of 1000. Loss AE    1767.26 | Loss Rev     611.81: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 399 | Batch: 960 of 1000. Loss AE    1888.67 | Loss Rev     694.16: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 400 | Batch: 960 of 1000. Loss AE    1666.96 | Loss Rev     500.39: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 401 | Batch: 960 of 1000. Loss AE    1610.02 | Loss Rev     476.91: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 402 | Batch: 960 of 1000. Loss AE    1597.54 | Loss Rev     475.75: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 403 | Batch: 960 of 1000. Loss AE    1591.61 | Loss Rev     478.51: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 404 | Batch: 960 of 1000. Loss AE    1590.28 | Loss Rev     478.64: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 405 | Batch: 960 of 1000. Loss AE    1589.88 | Loss Rev     482.07: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 406 | Batch: 960 of 1000. Loss AE    1590.58 | Loss Rev     481.26: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 407 | Batch: 960 of 1000. Loss AE    1584.50 | Loss Rev     481.51: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 408 | Batch: 960 of 1000. Loss AE    1584.37 | Loss Rev     480.76: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 409 | Batch: 960 of 1000. Loss AE    1581.53 | Loss Rev     479.13: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 410 | Batch: 960 of 1000. Loss AE    1581.51 | Loss Rev     478.49: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 411 | Batch: 960 of 1000. Loss AE    1581.11 | Loss Rev     478.94: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 412 | Batch: 960 of 1000. Loss AE    1578.95 | Loss Rev     480.38: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 413 | Batch: 960 of 1000. Loss AE    1578.61 | Loss Rev     475.51: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 414 | Batch: 960 of 1000. Loss AE    1579.60 | Loss Rev     477.80: 100%|██████████| 16/16 [00:29<00:00,  1.81s/it]\n",
            "Epoch 415 | Batch: 960 of 1000. Loss AE    1575.56 | Loss Rev     476.44: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 416 | Batch: 960 of 1000. Loss AE    1574.55 | Loss Rev     477.31: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 417 | Batch: 960 of 1000. Loss AE    1573.01 | Loss Rev     478.13: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 418 | Batch: 960 of 1000. Loss AE    1575.95 | Loss Rev     476.58: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 419 | Batch: 960 of 1000. Loss AE    1572.20 | Loss Rev     474.30: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 420 | Batch: 960 of 1000. Loss AE    1569.75 | Loss Rev     474.86: 100%|██████████| 16/16 [00:29<00:00,  1.85s/it]\n",
            "Epoch 421 | Batch: 960 of 1000. Loss AE    1566.65 | Loss Rev     473.60: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 422 | Batch: 960 of 1000. Loss AE    1567.00 | Loss Rev     474.30: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 423 | Batch: 960 of 1000. Loss AE    1566.17 | Loss Rev     475.60: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 424 | Batch: 960 of 1000. Loss AE    1565.76 | Loss Rev     473.55: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 425 | Batch: 960 of 1000. Loss AE    1563.31 | Loss Rev     472.54: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 426 | Batch: 960 of 1000. Loss AE    1562.14 | Loss Rev     473.62: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 427 | Batch: 960 of 1000. Loss AE    1562.08 | Loss Rev     475.13: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 428 | Batch: 960 of 1000. Loss AE    1559.44 | Loss Rev     472.62: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 429 | Batch: 960 of 1000. Loss AE    1559.76 | Loss Rev     473.62: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 430 | Batch: 960 of 1000. Loss AE    1559.97 | Loss Rev     471.84: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 431 | Batch: 960 of 1000. Loss AE    1553.77 | Loss Rev     470.08: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 432 | Batch: 960 of 1000. Loss AE    1553.93 | Loss Rev     471.42: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 433 | Batch: 960 of 1000. Loss AE    1554.26 | Loss Rev     471.64: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 434 | Batch: 960 of 1000. Loss AE    1553.43 | Loss Rev     473.44: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 435 | Batch: 960 of 1000. Loss AE    1552.39 | Loss Rev     470.62: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
            "Epoch 436 | Batch: 960 of 1000. Loss AE    1549.59 | Loss Rev     471.69: 100%|██████████| 16/16 [00:27<00:00,  1.71s/it]\n",
            "Epoch 437 | Batch: 960 of 1000. Loss AE    1550.62 | Loss Rev     472.17: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 438 | Batch: 960 of 1000. Loss AE    1550.68 | Loss Rev     472.50: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 439 | Batch: 960 of 1000. Loss AE    1547.07 | Loss Rev     472.57: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 440 | Batch: 960 of 1000. Loss AE    1545.26 | Loss Rev     470.59: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 441 | Batch: 960 of 1000. Loss AE    1541.95 | Loss Rev     472.71: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 442 | Batch: 960 of 1000. Loss AE    1541.50 | Loss Rev     470.63: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 443 | Batch: 960 of 1000. Loss AE    1542.09 | Loss Rev     471.01: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 444 | Batch: 960 of 1000. Loss AE    1540.14 | Loss Rev     470.49: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 445 | Batch: 960 of 1000. Loss AE    1538.27 | Loss Rev     467.80: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 446 | Batch: 960 of 1000. Loss AE    1538.18 | Loss Rev     468.86: 100%|██████████| 16/16 [00:29<00:00,  1.82s/it]\n",
            "Epoch 447 | Batch: 960 of 1000. Loss AE    1536.37 | Loss Rev     470.29: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 448 | Batch: 960 of 1000. Loss AE    1538.14 | Loss Rev     469.04: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 449 | Batch: 960 of 1000. Loss AE    1531.71 | Loss Rev     469.22: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 450 | Batch: 960 of 1000. Loss AE    1532.52 | Loss Rev     472.62: 100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
            "Epoch 451 | Batch: 960 of 1000. Loss AE    1530.77 | Loss Rev     471.34: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 452 | Batch: 960 of 1000. Loss AE    1530.35 | Loss Rev     474.33: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 453 | Batch: 960 of 1000. Loss AE    1527.59 | Loss Rev     473.97: 100%|██████████| 16/16 [00:29<00:00,  1.83s/it]\n",
            "Epoch 454 | Batch: 960 of 1000. Loss AE    1522.69 | Loss Rev     472.88: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 455 | Batch: 960 of 1000. Loss AE    1521.11 | Loss Rev     471.82: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 456 | Batch: 960 of 1000. Loss AE    1522.51 | Loss Rev     472.15: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 457 | Batch: 960 of 1000. Loss AE    1520.02 | Loss Rev     471.36: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 458 | Batch: 960 of 1000. Loss AE    1519.95 | Loss Rev     473.23: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 459 | Batch: 960 of 1000. Loss AE    1522.31 | Loss Rev     479.68: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 460 | Batch: 960 of 1000. Loss AE    1517.88 | Loss Rev     476.57: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 461 | Batch: 960 of 1000. Loss AE    1515.41 | Loss Rev     470.78: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 462 | Batch: 960 of 1000. Loss AE    1511.57 | Loss Rev     474.37: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 463 | Batch: 960 of 1000. Loss AE    1511.87 | Loss Rev     472.35: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 464 | Batch: 960 of 1000. Loss AE    1508.18 | Loss Rev     471.72: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 465 | Batch: 960 of 1000. Loss AE    1516.22 | Loss Rev     471.77: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 466 | Batch: 960 of 1000. Loss AE    1507.99 | Loss Rev     475.17: 100%|██████████| 16/16 [00:29<00:00,  1.85s/it]\n",
            "Epoch 467 | Batch: 960 of 1000. Loss AE    1507.48 | Loss Rev     471.04: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 468 | Batch: 960 of 1000. Loss AE    1513.15 | Loss Rev     476.44: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 469 | Batch: 960 of 1000. Loss AE    1505.64 | Loss Rev     475.64: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 470 | Batch: 960 of 1000. Loss AE    1503.93 | Loss Rev     475.61: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 471 | Batch: 960 of 1000. Loss AE    1495.27 | Loss Rev     470.47: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 472 | Batch: 960 of 1000. Loss AE    1500.37 | Loss Rev     474.63: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 473 | Batch: 960 of 1000. Loss AE    1495.97 | Loss Rev     472.40: 100%|██████████| 16/16 [00:27<00:00,  1.73s/it]\n",
            "Epoch 474 | Batch: 960 of 1000. Loss AE    1495.04 | Loss Rev     471.72: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 475 | Batch: 960 of 1000. Loss AE    1494.86 | Loss Rev     472.18: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 476 | Batch: 960 of 1000. Loss AE    1493.51 | Loss Rev     470.32: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 477 | Batch: 960 of 1000. Loss AE    1494.69 | Loss Rev     471.05: 100%|██████████| 16/16 [00:27<00:00,  1.74s/it]\n",
            "Epoch 478 | Batch: 960 of 1000. Loss AE    1491.19 | Loss Rev     470.50: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 479 | Batch: 960 of 1000. Loss AE    1489.35 | Loss Rev     469.96: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 480 | Batch: 960 of 1000. Loss AE    1490.26 | Loss Rev     468.11: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 481 | Batch: 960 of 1000. Loss AE    1489.55 | Loss Rev     470.59: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 482 | Batch: 960 of 1000. Loss AE    1485.62 | Loss Rev     468.04: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 483 | Batch: 960 of 1000. Loss AE    1478.63 | Loss Rev     470.02: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 484 | Batch: 960 of 1000. Loss AE    1483.43 | Loss Rev     469.56: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 485 | Batch: 960 of 1000. Loss AE    1499.04 | Loss Rev     481.38: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 486 | Batch: 960 of 1000. Loss AE    1482.01 | Loss Rev     468.12: 100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n",
            "Epoch 487 | Batch: 960 of 1000. Loss AE    1480.65 | Loss Rev     467.20: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 488 | Batch: 960 of 1000. Loss AE    1478.94 | Loss Rev     471.00: 100%|██████████| 16/16 [00:28<00:00,  1.80s/it]\n",
            "Epoch 489 | Batch: 960 of 1000. Loss AE    1476.07 | Loss Rev     468.00: 100%|██████████| 16/16 [00:28<00:00,  1.78s/it]\n",
            "Epoch 490 | Batch: 960 of 1000. Loss AE    1481.70 | Loss Rev     474.97: 100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n",
            "Epoch 491 | Batch: 960 of 1000. Loss AE    1507.92 | Loss Rev     494.98: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 492 | Batch: 960 of 1000. Loss AE    1476.82 | Loss Rev     463.71: 100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n",
            "Epoch 493 | Batch: 960 of 1000. Loss AE    1486.28 | Loss Rev     479.25: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 494 | Batch: 960 of 1000. Loss AE    1477.26 | Loss Rev     469.58: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
            "Epoch 495 | Batch: 960 of 1000. Loss AE    1470.21 | Loss Rev     463.47: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 496 | Batch: 960 of 1000. Loss AE    1476.65 | Loss Rev     472.66: 100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n",
            "Epoch 497 | Batch: 960 of 1000. Loss AE    1468.29 | Loss Rev     465.22: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n",
            "Epoch 498 | Batch: 960 of 1000. Loss AE    1472.37 | Loss Rev     473.01: 100%|██████████| 16/16 [00:29<00:00,  1.87s/it]\n",
            "Epoch 499 | Batch: 960 of 1000. Loss AE    1487.28 | Loss Rev     485.61: 100%|██████████| 16/16 [00:27<00:00,  1.75s/it]\n",
            "Epoch 500 | Batch: 960 of 1000. Loss AE    1468.16 | Loss Rev     470.80: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVuhFpcGH7z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "360fc9d6-af40-43e4-ce3a-5fe3c0639354"
      },
      "source": [
        "autoencoder_model.load_weights('/content/gdrive/MyDrive/imagestegan/model/stackedmodel.hdf5')\n",
        "# Retrieve decoded predictions.\n",
        "decoded = autoencoder_model.predict([input_S, input_C])\n",
        "decoded_S, decoded_C = decoded[...,0:3], decoded[...,3:6]\n",
        "\n",
        "# Get absolute difference between the outputs and the expected values.\n",
        "diff_S, diff_C = np.abs(decoded_S - input_S), np.abs(decoded_C - input_C) \n",
        "\n",
        "def pixel_errors(input_S, input_C, decoded_S, decoded_C):\n",
        "    \"\"\"Calculates mean of Sum of Squared Errors per pixel for cover and secret images. \"\"\"\n",
        "    see_Spixel = np.sqrt(np.mean(np.square(255*(input_S - decoded_S))))\n",
        "    see_Cpixel = np.sqrt(np.mean(np.square(255*(input_C - decoded_C))))\n",
        "    \n",
        "    return see_Spixel, see_Cpixel\n",
        "\n",
        "def pixel_histogram(diff_S, diff_C):\n",
        "    \"\"\"Calculates histograms of errors for cover and secret image. \"\"\"\n",
        "    diff_Sflat = diff_S.flatten()\n",
        "    diff_Cflat = diff_C.flatten()\n",
        "    \n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    a=fig.add_subplot(1,2,1)\n",
        "        \n",
        "    imgplot = plt.hist(255* diff_Cflat, 100, density=1, alpha=0.75, facecolor='red')\n",
        "    a.set_title('Distribution of error in the Cover image.')\n",
        "    plt.axis([0, 250, 0, 0.2])\n",
        "    \n",
        "    a=fig.add_subplot(1,2,2)\n",
        "    imgplot = plt.hist(255* diff_Sflat, 100, density=1, alpha=0.75, facecolor='red')\n",
        "    a.set_title('Distribution of errors in the Secret image.')\n",
        "    plt.axis([0, 250, 0, 0.2])\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "# Print pixel-wise average errors in a 256 scale.\n",
        "S_error, C_error = pixel_errors(input_S, input_C, decoded_S, decoded_C)\n",
        "\n",
        "print (\"S error per pixel [0, 255]:\", S_error)\n",
        "print (\"C error per pixel [0, 255]:\", C_error)\n",
        "\n",
        "pixel_histogram(diff_S, diff_C)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S error per pixel [0, 255]: 5.934085798601941\n",
            "C error per pixel [0, 255]: 9.25540583496071\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAE/CAYAAAAXN63eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5wldX3n+9fbmWH8wYgKE4MDCAYiwZhgbNC7iYTZREVXGe4NKlxX0ZAQo+xu1ugVE01YomvYvYl7vctNxIgKKkiIXCeJLprroJvsgtOjCAw80BEIzID8/qUozMDn/lHVcjh0T58z092nT9fr+XjUo8+p+tb3fOt7qs/nfKq+VSdVhSRJkiRp6XvSqBsgSZIkSVoYJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAHZHkL5O8f47qOiDJD5Isa59fmuS35qLutr4vJTlpruob4nU/kOTOJN9f6NceRJKXJbluDuv7ZJIPzFV9i9Wo9idJi5PxcKDXXdTxcGeSbE5y9BzVdXSSrXNR17iY6+8aWpxMAJeAJDcm+VGSB5Lcm+R/JHlbkp+8v1X1tqr6kwHr+vWdlamqm6pqz6p6ZA7afnqST/fV/6qq+tTu1j1kOw4Afh84rKp+eiFfe1BV9d+r6vm7sm6StyT5x7luU99rvDLJ19v98I4kX0ty7Hy+5iBGsT9JGg3j4e4bh3i4M1X1gqq6dFfWTVJJDp7jJk3VvUeSP0uytT1ocGOS/zIfrzVLO3a6X+/Odw2NDxPApeO1VbUKeC7wp8B7gI/P9YskWT7XdS4SBwB3VdXtu1vRdH00bL+NWz8nOR74a+BcYD/g2cAfAa9d4HYsW8jXk7QoGQ93j/FwfrwXmACOBFYBRwPfnMsXWEJ9pflWVU5jPgE3Ar/eN+9I4FHg59vnnwQ+0D7eB/g74F7gbuC/0xwMOK9d50fAD4D/AzgQKOBk4Cbg6z3zlrf1XQp8CPgGcD/wBeBZ7bKjga3TtRc4BngY2N6+3rd76vut9vGTgPcB/wzcTpNg7NUum2rHSW3b7gT+cCf9tFe7/h1tfe9r6//1dpsfbdvxyRnWfw1wRdtv/wP4hb5teg9wJfAQcPA0/TbItvyk/DSv/7i+bF/zXe1r3gd8DnjyNOv9HPBj4JF2++7t2SfOAv4eeAC4HPiZnvUOBb5Cs49cB7x+hn5J2+Z376Tvd7btXwJO7Sv/beB/m60d7Tb8BfBF4If0/R9Msz+9Bfgn4MPt+3g98C/a+Te3bTupZ91/BXyLZr++GTi9r+43t9t0F/B+ev4X220+Dfheu/xC2v8LJyen+ZkwHi7JeAg8Gfh0+1l6L7ARePZs+wBwOs1n77k0cW4zMDHDel9vX/eH7ba/Yeo9ozkjejtwK/DWnnVWAv9n287bgL8EnjJD/X8H/N5O3pPnAH/Tvic3AP+2Z9ky4A9o4skDwCZg/3ZZAe8AvgvcsLP3h2n262nacTRP/K7x7vb9/CHNwZRn08TuB4B/AJ7ZU/6vge/TfC/5OvCCnmV7A39L87+xEfgA8I89ywf63uE0B5+Vo26A0xy8idMEvHb+TcDvto8/yWMB70Pth9SKdnoZkOnq4rEP4nOBpwFPYfqAtw34+bbM3wCfbpc97oOk/zVoPpw/3bf8Uh4LeL8JbAGeB+wJfB44r69tH2vb9Ys0webnZuinc2mC8ap23e8AJ8/Uzr51X0Tz4f8Smg/ik9rtWNmzTVcA+/f1UW+/DbItPyk/TRse18b2Nb9BEzSeBVwLvG2G9r+Fng/Znn3iLpovR8uBzwAXtMueRpPwvLVd9iKaLxSHTVP3oW3bD9pJ/+1s298M/FNP2cNogtbK2drRbsN9wC/TfKGYLgG+lMcngDva+pbRBJ+baBLhlcAraALanj19/sK27l+gCfDH9bTzB8CvAHvQfAnYzmP79r8DLqM5I7oS+Chw/qg/L5yclvKE8XBJxkPgd2gSh6e2r/li4Omz7QNtn/4YeHW73oeAy3aybQUc3PP8aJqYcQbN/vFq4EHahIfmYOJ6mhi8qm3jh2ao+300++HbaeJKepY9iSap+yOaePI8mgOUr2yXvxu4Cng+zUHXXwT27mnzV9o2PGXA9+cJ/yN929z/XeMymqRvTVv3N9vXeTLwVeCPe8r/ZtsXK4H/AlzRs+yCdnoqTQy9mfa7CUN873Da/ckhoEvbLTQfCP22A/sCz62q7dWM965Z6jq9qn5YVT+aYfl5VXV1Vf2Q5kzI6+doON4bgT+vquur6gc0QyhO6Bvm8B+q6kdV9W2aM0e/2F9J25YTgPdW1QNVdSPwZ8CbBmzHKcBHq+ryqnqkmmsyHgJe2lPmI1V1c18f9fbbINsyWz/3+0hV3VJVd9MEnsMHXG/KxVX1jaraQZMATq3/GuDGqvpEVe2oqm/RfJF53TR17N3+vXUnr7Ozbb8YODzJc3vKfr6qHhqwHV+oqn+qqker6scDbPMNbX2P0Jw13R84o6oeqqov0xyFPxigqi6tqqvauq8Ezgd+ta3neOBvq+ofq+phmsDd+3/0Npoj8FvbbTkdON4hOtJIGA9bYxoPt9PEmoPb19xUVfcP2N5/rKovtp/55zFNn8xiO02M2F5VX6Q58Pf8JGn74t9X1d1V9QDwH2n6djofAs6k2fZJYFvPDX6OAFZX1RlV9XBVXU+TzE/V9VvA+6rqump8u6ru6q27bcOPGOz9Gdb/XVW3VdU2mrPkl1fVt9qYezFNsgZAVZ3T7ldTce8Xk+zV7ne/QZMsPlhV1wC917cO871Du8kEcGlbQ3Mavd9/pjny9uUk1yc5bYC6bh5i+T/THCnbZ6BW7txz2vp6615OcyRqSu9dyh6kOZrYb5+2Tf11rRmwHc8Ffr+9qcC9Se6lSRye01Nmuj7qnTfItszWz/0G2fZdWf+5wEv6tveNwHQ3BJgKQvvu5HVm3PY2aP49jwW6E2mS0UHbMWyf3dbz+EcAVdU/b0+AJC9JsqG9qc19NEnd1H79nN7XrqoHeawvptp+cU+7r6UZhtv7fktaGMbDx4xjPDwPuAS4IMktSf5TkhUDtre/T5485IG4u9qDpL117AmspjmTtamnH/5bO/8J2mTsrKr6ZeAZwAeBc5L8HE2fPqevT/+Ax/pjf5rhnzPp7atB3p9h9cfImWLmsiR/muR7Se6nOXsIzT63muY97m1rf7sH/d6h3WQCuEQlOYLmw/wJd35sj8z8flU9DzgWeGeSX5taPEOVsx0R3b/n8QE0R8zupBkv/tSedi3j8R+Os9V7C82HQm/dO3j8h88g7mzb1F/XtgHXvxn4YFU9o2d6alWd31Nmum3pnTfItszWH7tq2HpvBr7Wt717VtXvTlP2urb8b+ykvtm2/XzgxCT/C82Qkg1DtGO++gzgszTDe/avqr1ohoqlXXYrzfBOAJI8hcfOhk61/VV9bX9yewRV0gIxHj7B2MXD9uzbf6iqw2iu234NzeUDo3QnTfLzgp5+2KuqZj0Q256lPQu4h8eGQt7Q16erqurV7So3Az+zsyp7Hs/2/sxnzPzfgXU015LuRTOcF5q4eQfNe7xfT/ne/5VhvndoN5kALjFJnp7kNTRjrD9dVVdNU+Y1SQ5uhy/cR3NW4tF28W00Y8+H9a+THJbkqTRj5S9qh1t8h+Zo279qj9a9j2Zc+JTbgAN7b9Hd53zg3yc5KMmeNMMrPtd3NG5WbVsuBD6YZFU73PCdNBeVD+JjwNvaM0JJ8rR2m1YN0Yw52ZZddBuwX5I9Biz/d8DPJnlTkhXtdER7pPJxqqpo+vL9Sd7a7oNPSvIrSc5ui8227V+k+TJwRjt/an8cuB3zZBVwd1X9OMmRNMFtykXAa5P8i7ZfT+ex5BCaZPGDU0Nbk6xOsm6B2i11nvFweuMYD5OsTfLCNmm+nyaBfXS6srtp4Pe8jVMfAz6c5Kfadq5J8srpyif5vTS/K/iUJMvTDP9cRXOjsW8ADyR5T7t8WZKfbw9eAPwV8CdJDmn7/BeS7D3d6zD7+7Or+/UgVtEMN72L5mDHf5xa0O53nwdOT/LUJIfy+CR+1PG+U0wAl46/TfIAzRGUPwT+nOZC2ukcQnPXph8A/xP4f6pq6ozLh4D3pTn9/q4hXv88mgvrv09zBuffAlTVfTQXPP8VzdHFH9LcUWvKX7d/70oy3e2Qz2nr/jrNXbF+DPybIdrV69+0r389zZHgz7b1z6qqJoHfBv4rzRG7LTQ3FBnGXG7LsL5Kc/ez7ye5c7bC7bDMV9AMy7yF5n09k8d/WektfxHNHdN+sy1/G80NVr7QFtnptrfXCnye5qjhZ3e1HfPg7cAZ7f/WH9F8aZpq22aabbiA5mzgD2gujn+oLfJ/0Zw9/HK7/mU0F+UDkOZ3oF62EBshdYzxcHbjFg9/muag2/00w+m/1q4/104HPtW+568foPx7aLb/sjRDHv+B5kYt03mQ5lrL79OcPXwH8BvVXAf5CM1ZzcNp+uNOmv1kr3bdP6eJP1+m6YOP09zw5QkGeH92db8exLk0w3m3AdfQxL1ep9Js0/dp3r/zaWPmbPE+yR8k+dIct7ezpu50JUnaDe1R7HuBQ6rqhlG3R5KkxSzJmcBPV9VJsxbWnPIMoCTtoiSvbYeyPI3mZyCu4rGL3iVJUivJoe3w1bSXVZxMcxdRLbCBEsAkxyS5LsmWTHOHrCTvTHJNkiuT/H957HbuJDkpyXfb6aSe+S9OclVb50fa8feSNE7W0QxVuYVmKNkJ5bCKzjFGStJAVtFc7vFDmp9h+jMeu1REC2jWIaDtBbffAV5OM1Z9I3Bi+/sdU2XW0vwmyINJfhc4uqrekORZNL91MkFz16FNwIur6p4k36AZF385zQ0gPlJVju2VJI0NY6QkadwMcgbwSGBLe5HqwzQ3PHjcneyqakP7G1jQXPA5dYvXVwJfqebHKe8BvgIck2Rf4OlVdVl7tPxc4Lg52B5JkhaSMVKSNFYGSQDX8PgfatzKzn8s9GRg6ijlTOuu4fF3vpqtTkmSFiNjpCRprCyfy8qS/GuaoSy/Ood1ngKcAvC0pz3txYceeuhcVS1JWqQ2bdp0Z1Wtnr3k+JjrGPm4+PikJ7340Ke2vzH+/JnuQi9JWgp2N0YOkgBuA/bveb5fO+9xkvw6ze/t/Gr7m15T6x7dt+6l7fz9+uY/oU6AqjobOBtgYmKiJicnB2iyJGmcJfnnUbdhQCOLkY+Lj6tW1eTERLNgw4b+opKkJWR3Y+QgQ0A3AockOSjJHjQ/0Li+rxEvAj4KHFtVt/csugR4RZJnJnkmzQ88XlJVtwL3J3lpe2ezN+NdgCRJ48cYKUkaK7OeAayqHUlOpQlUy4BzqmpzkjOAyapaD/xnYE/gr9s7Vd9UVcdW1d1J/oQmQAKcUVV3t4/fDnwSeArN9RDe3UySNFaMkZKkcTPrz0AsJg4BlaRuSLKpqiZG3Y5x4RBQSeqO3Y2RA/0QvCRJkiRp/JkASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSRwyUACY5Jsl1SbYkOW2a5Ucl+WaSHUmO75m/NskVPdOPkxzXLvtkkht6lh0+d5slSdL8Mz5KksbN8tkKJFkGnAW8HNgKbEyyvqqu6Sl2E/AW4F2961bVBuDwtp5nAVuAL/cUeXdVXbQ7GyBJ0igYHyVJ42jWBBA4EthSVdcDJLkAWAf8JMBV1Y3tskd3Us/xwJeq6sFdbq0kSYuH8VGSNHYGGQK6Bri55/nWdt6wTgDO75v3wSRXJvlwkpW7UKckSaNifJQkjZ0FuQlMkn2BFwKX9Mx+L3AocATwLOA9M6x7SpLJJJN33HHHvLdVkqSFMmfxcfv2eW+rJGlpGCQB3Abs3/N8v3beMF4PXFxVP4lQVXVrNR4CPkEzlOYJqursqpqoqonVq1cP+bKSJM2bxRMfV6wY8mUlSV01SAK4ETgkyUFJ9qAZqrJ+yNc5kb7hLe1RT5IEOA64esg6JUkaJeOjJGnszJoAVtUO4FSa4SnXAhdW1eYkZyQ5FiDJEUm2Aq8DPppk89T6SQ6kOUL6tb6qP5PkKuAqYB/gA7u/OZIkLQzjoyRpHKWqRt2GgU1MTNTk5OSomyFJmmdJNlXVxKjbMS4mVq2qyYm2uzZsGG1jJEnzandj5ILcBEaSJEmSNHomgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEDJYBJjklyXZItSU6bZvlRSb6ZZEeS4/uWPZLkinZa3zP/oCSXt3V+Lskeu785kiQtHOOjJGnczJoAJlkGnAW8CjgMODHJYX3FbgLeAnx2mip+VFWHt9OxPfPPBD5cVQcD9wAn70L7JUkaCeOjJGkcDXIG8EhgS1VdX1UPAxcA63oLVNWNVXUl8OggL5okwL8ELmpnfQo4buBWS5I0esZHSdLYGSQBXAPc3PN8aztvUE9OMpnksiRTQWxv4N6q2rGLdUqSNGrGR0nS2Fm+AK/x3KraluR5wFeTXAXcN+jKSU4BTgE44IAD5qmJkiQtuLmLjytXzlMTJUlLzSBnALcB+/c836+dN5Cq2tb+vR64FHgRcBfwjCRTCeiMdVbV2VU1UVUTq1evHvRlJUmab4snPq5YMXzrJUmdNEgCuBE4pL0r2R7ACcD6WdYBIMkzk6xsH+8D/DJwTVUVsAGYuiPaScAXhm28JEkjZHyUJI2dWRPA9jqEU4FLgGuBC6tqc5IzkhwLkOSIJFuB1wEfTbK5Xf3ngMkk36YJaH9aVde0y94DvDPJFpprHj4+lxsmSdJ8Mj5KksZRmoON42FiYqImJydH3QxJ0jxLsqmqJkbdjnExsWpVTU603bVhw2gbI0maV7sbIwf6IXhJkiRJ0vgbrwTwuutg7dpRt0KSJEmSxtJ4JYCSJEmSpF1mAihJkiRJHbEQPwQvSZIWSu+lEt4QRpLUxzOAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEDJYBJjklyXZItSU6bZvlRSb6ZZEeS43vmH57kfybZnOTKJG/oWfbJJDckuaKdDp+bTZIkaWEYHyVJ42b5bAWSLAPOAl4ObAU2JllfVdf0FLsJeAvwrr7VHwTeXFXfTfIcYFOSS6rq3nb5u6vqot3dCEmSFprxUZI0jmZNAIEjgS1VdT1AkguAdcBPAlxV3dgue7R3xar6Ts/jW5LcDqwG7kWSpPFmfJQkjZ1BhoCuAW7ueb61nTeUJEcCewDf65n9wXboy4eTrBy2TkmSRsj4KEkaOwtyE5gk+wLnAW+tqqmjoO8FDgWOAJ4FvGeGdU9JMplk8o7t2xeiuZIkLQjjoyRpoQ2SAG4D9u95vl87byBJng78PfCHVXXZ1PyqurUaDwGfoBlK8wRVdXZVTVTVxOoVKwZ9WUmS5pvxUZI0dgZJADcChyQ5KMkewAnA+kEqb8tfDJzbfzF7e9STJAGOA64epuGSJI2Y8VGSNHZmTQCragdwKnAJcC1wYVVtTnJGkmMBkhyRZCvwOuCjSTa3q78eOAp4yzS3s/5MkquAq4B9gA/M6ZZJkjSPjI+SpHGUqhp1GwY2sWpVTU5MwIYNo26KJGkeJdlUVROjbse4+El87Ge8lKQlZ3dj5ILcBEaSJEmSNHomgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUEctH3YBdsnbtY4+9w5kkSZIkDcQzgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUESaAkiRJktQRJoCSJEmS1BEmgJIkSZLUEQMlgEmOSXJdki1JTptm+VFJvplkR5Lj+5adlOS77XRSz/wXJ7mqrfMjSbL7myNJ0sIyRkqSxsmsCWCSZcBZwKuAw4ATkxzWV+wm4C3AZ/vWfRbwx8BLgCOBP07yzHbxXwC/DRzSTsfs8lZIkjQCxkhJ0rgZ5AzgkcCWqrq+qh4GLgDW9Raoqhur6krg0b51Xwl8parurqp7gK8AxyTZF3h6VV1WVQWcCxy3uxsjSdICM0ZKksbKIAngGuDmnudb23mDmGndNe3jXalTkqTFwhgpSRori/4mMElOSTKZZPKO7dtH3RxJkhYF46MkaVcMkgBuA/bveb5fO28QM627rX08a51VdXZVTVTVxOoVKwZ8WUmSFsTIYqTxUZK0KwZJADcChyQ5KMkewAnA+gHrvwR4RZJnthe2vwK4pKpuBe5P8tL2zmZvBr6wC+2XJGmUjJGSpLEyawJYVTuAU2kC1bXAhVW1OckZSY4FSHJEkq3A64CPJtncrns38Cc0AXIjcEY7D+DtwF8BW4DvAV+a0y2TJGmeGSMlSeMmzQ3GxsPEqlU1OTHx+JkbNoymMZKkeZNkU1VNzF5SMEN8BGOkJC1BuxsjF/1NYCRJkiRJc8MEUJIkSZI6wgRQkiRJkjrCBFCSJEmSOsIEUJIkSZI6wgRQkiRJkjrCBFCSJEmSOsIEUJIkSZI6wgRQkiRJkjrCBFCSJEmSOsIEUJIkSZI6YvwTwLVrm0mSJEmStFPjnwBKkiRJkgZiAihJkiRJHWECKEmSJEkdYQIoSZIkSR1hAihJkiRJHWECKEmSJEkdYQIoSZIkSR1hAihJkiRJHWECKEmSJEkdYQIoSZIkSR1hAihJkiRJHWECKEnSUrV2bTNJktQyAZQkSZKkjhgoAUxyTJLrkmxJcto0y1cm+Vy7/PIkB7bz35jkip7p0SSHt8subeucWvZTc7lhkiTNN+OjJGnczJoAJlkGnAW8CjgMODHJYX3FTgbuqaqDgQ8DZwJU1Weq6vCqOhx4E3BDVV3Rs94bp5ZX1e1zsD2SJC0I46MkaRwNcgbwSGBLVV1fVQ8DFwDr+sqsAz7VPr4I+LUk6StzYruuJElLgfFRkjR2BkkA1wA39zzf2s6btkxV7QDuA/buK/MG4Py+eZ9oh7e8f5qAKEnSYmZ8lCSNnQW5CUySlwAPVtXVPbPfWFUvBF7WTm+aYd1Tkkwmmbxj+/YFaK0kSQvD+ChJWmiDJIDbgP17nu/Xzpu2TJLlwF7AXT3LT6Dv6GZVbWv/PgB8lmYozRNU1dlVNVFVE6tXrBiguZIkLQjjoyRp7AySAG4EDklyUJI9aILV+r4y64GT2sfHA1+tqgJI8iTg9fRc35BkeZJ92scrgNcAVyNJ0vgwPkqSxs7y2QpU1Y4kpwKXAMuAc6pqc5IzgMmqWg98HDgvyRbgbpogOOUo4Oaqur5n3krgkja4LQP+AfjYnGyRJEkLwPgoSRpHaQ9EjoWJVatqcmJi+oUbNixsYyRJ8ybJpqqa4QNf/XYaH8EYKUlLyO7GyAW5CYwkSZIkafRMACVJkiSpI0wAJUmSJKkjTAAlSZIkqSNMACVJkiSpI0wAJUmSJKkjTAAlSZIkqSOWTgK4dm0zSZIkSZKmtXQSQEmSJEnSTpkASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHmABKkiRJUkeYAEqSJElSR5gASpIkSVJHDJQAJjkmyXVJtiQ5bZrlK5N8rl1+eZID2/kHJvlRkiva6S971nlxkqvadT6SJHO1UZIkLRRjpCRpnMyaACZZBpwFvAo4DDgxyWF9xU4G7qmqg4EPA2f2LPteVR3eTm/rmf8XwG8Dh7TTMbu+GZIkLTxjpCRp3AxyBvBIYEtVXV9VDwMXAOv6yqwDPtU+vgj4tZ0drUyyL/D0qrqsqgo4Fzhu6NZLkjRaxkhJ0lgZJAFcA9zc83xrO2/aMlW1A7gP2LtddlCSbyX5WpKX9ZTfOkudkiQtdsZISdJYWT7P9d8KHFBVdyV5MfD/JnnBMBUkOQU4BeCAlStnX2Ht2sceb9gwzEtJkrSQditGDh0fJUlisDOA24D9e57v186btkyS5cBewF1V9VBV3QVQVZuA7wE/25bfb5Y6adc7u6omqmpi9YoVAzRXkqQFM7IYaXyUJO2KQRLAjcAhSQ5KsgdwArC+r8x64KT28fHAV6uqkqxuL5AnyfNoLmS/vqpuBe5P8tL2Oog3A1+Yg+2RJGkhGSMlSWNl1iGgVbUjyanAJcAy4Jyq2pzkDGCyqtYDHwfOS7IFuJsmAAIcBZyRZDvwKPC2qrq7XfZ24JPAU4AvtZMkSWPDGClJGjdpbjA2HiZWrarJiYnBV/AaQEkaS0k2VdUQH/jdNmt8NB5K0pKxuzFyvm8CI0mSRs0bpEmSWoNcAyhJkiRJWgJMACVJkiSpI0wAJUmSJKkjTAAlSZIkqSNMACVJkiSpI0wAJQL/MqAAAAn7SURBVEmSJKkjTAAlSZIkqSNMACVJkiSpI0wAJUmSJKkjlnYCuHZtM0mSJEmSlngCKEmSJEn6CRNASZIkSeoIE0BJkiRJ6ggTQEmSJEnqCBNASZIkSeoIE0BJkiRJ6ggTQEmSJEnqCBNASZIkSeoIE0BJkiRJ6ggTQEmSJEnqCBNASZIkSeoIE0BJkiRJ6ohuJIBr1zaTJEldZ0yUpE7rRgIoSZIkSRosAUxyTJLrkmxJcto0y1cm+Vy7/PIkB7bzX55kU5Kr2r//smedS9s6r2inn5qrjZIkaSEYHyVJ42b5bAWSLAPOAl4ObAU2JllfVdf0FDsZuKeqDk5yAnAm8AbgTuC1VXVLkp8HLgHW9Kz3xqqanKNtkSRpwRgfJUnjaJAzgEcCW6rq+qp6GLgAWNdXZh3wqfbxRcCvJUlVfauqbmnnbwaekmTlXDRckqQRMz5KksbOIAngGuDmnudbefxRyseVqaodwH3A3n1lfgP4ZlU91DPvE+3wlvcnyVAtlyRptIyPkqSxsyA3gUnyApphL7/TM/uNVfVC4GXt9KYZ1j0lyWSSyTu2b5//xkqStECMj5KkhTZIArgN2L/n+X7tvGnLJFkO7AXc1T7fD7gYeHNVfW9qhara1v59APgszVCaJ6iqs6tqoqomVq9YMcg2SZK0EIyPkqSxM0gCuBE4JMlBSfYATgDW95VZD5zUPj4e+GpVVZJnAH8PnFZV/zRVOMnyJPu0j1cArwGu3r1NGcDUbx/5+0eSpN23dOKjJKkzZk0A22sWTqW5Q9m1wIVVtTnJGUmObYt9HNg7yRbgncDUrbBPBQ4G/qjvdtYrgUuSXAlcQXOE9GNzuWGSJM0n46MkaRylqkbdhoFNrFpVkxMTc1PZhg1zU48kac4l2VRVc/SBv/TtUnw0DkrSWNrdGLkgN4GRJEmSJI2eCaAkSZIkdcTyUTdAkiSNQO8N0RwOKkmd4RlASZIkSeqI7iaA/hyEJEmSpI7pbgIoSZIkSR1jAihJkiRJHWECKEmSJEkdYQIoSZIkSR1hAihJkiRJHWEC6N1AJUmSJHWECaAkSV3nwVBJ6gwTQEmSJEnqCBNASZIkSeqI5aNuwKLRO/Rlw4bRtUOSJEmS5olnACVJkiSpIzwDKEmSGo6GkaQlzzOA0/FuaJIkSZKWIBNASZIkSeoIE0BJkiRJ6givAdwZr4WQJHWVMVCSliTPAEqSJElSR5gADsobw0iSusoYKElLhgngsAyCkiRJksaU1wBKkqTBeF2gJI29gc4AJjkmyXVJtiQ5bZrlK5N8rl1+eZIDe5a9t51/XZJXDlrnojd1JrB3kiR1TmdjpLFPksbSrGcAkywDzgJeDmwFNiZZX1XX9BQ7Gbinqg5OcgJwJvCGJIcBJwAvAJ4D/EOSn23Xma3O8TMVCD0qKkmdYIzEs4KSNGYGGQJ6JLClqq4HSHIBsA7oDUTrgNPbxxcB/zVJ2vkXVNVDwA1JtrT1MUCd42vQI6IGSkkad8bIXtPFP2OdJC0qgySAa4Cbe55vBV4yU5mq2pHkPmDvdv5lfeuuaR/PVufSN5dDZwywkjQKxsjZ7EqsM6ZJ0rxZ9DeBSXIKcEr79KFceunVo2zPopVMN3cf4M4Fbsk4s7+GY38Nx/4azvNH3YDFbknHx+lj2lzy/3F49tlw7K/h2F/D2a0YOUgCuA3Yv+f5fu286cpsTbIc2Au4a5Z1Z6sTgKo6GzgbIMlkVU0M0GZhfw3L/hqO/TUc+2s4SSZH3YYBjSxGGh93nf01PPtsOPbXcOyv4exujBzkLqAbgUOSHJRkD5oL1tf3lVkPnNQ+Ph74alVVO/+E9g5oBwGHAN8YsE5JkhY7Y6QkaazMegawvV7hVOASYBlwTlVtTnIGMFlV64GPA+e1F7DfTROsaMtdSHPh+g7gHVX1CMB0dc795kmSNH+MkZKkcZPmIOR4SHJKO+RFA7C/hmN/Dcf+Go79NRz7azj213Dsr+HZZ8Oxv4Zjfw1nd/trrBJASZIkSdKuG+QaQEmSJEnSEjAWCWCSY5Jcl2RLktNG3Z7FKMmNSa5KcsXUnYGSPCvJV5J8t/37zFG3c5SSnJPk9iRX98ybto/S+Ei7z12Z5JdG1/LRmKG/Tk+yrd3Prkjy6p5l723767okrxxNq0cnyf5JNiS5JsnmJP+une8+No2d9Jf72JCMkbMzRu6c8XE4xsfhGB+HsyDxsaoW9URzAfz3gOcBewDfBg4bdbsW2wTcCOzTN+8/Aae1j08Dzhx1O0fcR0cBvwRcPVsfAa8GvgQEeClw+ajbv0j663TgXdOUPaz931wJHNT+zy4b9TYscH/tC/xS+3gV8J22X9zHhusv97Hh+tEYOVg/GSN33j/Gx93vLz+7Zu4v4+Pc9Nec7WPjcAbwSGBLVV1fVQ8DFwDrRtymcbEO+FT7+FPAcSNsy8hV1ddp7sDXa6Y+WgecW43LgGck2XdhWro4zNBfM1kHXFBVD1XVDcAWmv/dzqiqW6vqm+3jB4BrgTW4j01rJ/01k87vYzMwRu46Y2TL+Dgc4+NwjI/DWYj4OA4J4Brg5p7nW9l5J3RVAV9OsinJKe28Z1fVre3j7wPPHk3TFrWZ+sj9bmantkMyzukZMmV/9UhyIPAi4HLcx2bV11/gPjYM+2Uwxsjh+dk1PD+7ZmF8HM58xcdxSAA1mF+pql8CXgW8I8lRvQurOUfsLV93wj4ayF8APwMcDtwK/Nlom7P4JNkT+Bvg96rq/t5l7mNPNE1/uY9pPhgjd4P9MxA/u2ZhfBzOfMbHcUgAtwH79zzfr52nHlW1rf17O3Axzanf26ZOmbd/bx9dCxetmfrI/W4aVXVbVT1SVY8CH+OxIQb2F5BkBc2H9Weq6vPtbPexGUzXX+5jQ7NfBmCM3CV+dg3Bz66dMz4OZ77j4zgkgBuBQ5IclGQP4ARg/YjbtKgkeVqSVVOPgVcAV9P000ltsZOAL4ymhYvaTH20HnhzeyeqlwL39QxT6Ky+Mfj/K81+Bk1/nZBkZZKDgEOAbyx0+0YpSYCPA9dW1Z/3LHIfm8ZM/eU+NjRj5CyMkbvMz64h+Nk1M+PjcBYiPi6f2ybPvarakeRU4BKau52dU1WbR9ysxebZwMXN/sJy4LNV9d+SbAQuTHIy8M/A60fYxpFLcj5wNLBPkq3AHwN/yvR99EWau1BtAR4E3rrgDR6xGfrr6CSH0wzTuBH4HYCq2pzkQuAaYAfwjqp6ZBTtHqFfBt4EXJXkinbeH+A+NpOZ+utE97HBGSMHYoychfFxOMbHoRkfhzPv8THt7UMlSZIkSUvcOAwBlSRJkiTNARNASZIkSeoIE0BJkiRJ6ggTQEmSJEnqCBNASZIkSeoIE0BJkiRJ6ggTQEmSJEnqCBNASZIkSeqI/x8b6Ii7W40eegAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnBLYxQmQj5y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GduFEftGT57"
      },
      "source": [
        "# Show images in gray scale\n",
        "SHOW_GRAY = False\n",
        "# Show difference bettwen predictions and ground truth.\n",
        "SHOW_DIFF = True\n",
        "\n",
        "# Diff enhance magnitude\n",
        "ENHANCE = 1\n",
        "\n",
        "# Number of secret and cover pairs to show.\n",
        "n = 6\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "def show_image(img, n_rows, n_col, idx, gray=False, first_row=False, title=None):\n",
        "    ax = plt.subplot(n_rows, n_col, idx)\n",
        "    if gray:\n",
        "        plt.imshow(rgb2gray(img), cmap = plt.get_cmap('gray'))\n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if first_row:\n",
        "        plt.title(title)\n",
        "\n",
        "plt.figure(figsize=(14, 15))\n",
        "rand_indx = [random.randint(0, 1000) for x in range(n)]\n",
        "# for i, idx in enumerate(range(0, n)):\n",
        "for i, idx in enumerate(rand_indx):\n",
        "    n_col = 6 if SHOW_DIFF else 4\n",
        "    \n",
        "    show_image(input_C[idx], n, n_col, i * n_col + 1, gray=SHOW_GRAY, first_row=i==0, title='Cover')\n",
        "\n",
        "    show_image(input_S[idx], n, n_col, i * n_col + 2, gray=SHOW_GRAY, first_row=i==0, title='Secret')\n",
        "    \n",
        "    show_image(decoded_C[idx], n, n_col, i * n_col + 3, gray=SHOW_GRAY, first_row=i==0, title='Encoded Cover')\n",
        "    \n",
        "    show_image(decoded_S[idx], n, n_col, i * n_col + 4, gray=SHOW_GRAY, first_row=i==0, title='Decoded Secret')\n",
        "\n",
        "    \n",
        "    if SHOW_DIFF:\n",
        "        show_image(np.multiply(diff_C[idx], ENHANCE), n, n_col, i * n_col + 5, gray=SHOW_GRAY, first_row=i==0, title='Diff Cover')\n",
        "        \n",
        "        show_image(np.multiply(diff_S[idx], ENHANCE), n, n_col, i * n_col + 6, gray=SHOW_GRAY, first_row=i==0, title='Diff Secret')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jORBVa7VQynt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP-dvylxRBM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShB-UaYuRP6s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuM-NiEURef9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jDWm26gRtNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}